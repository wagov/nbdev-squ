{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# api\n",
    "\n",
    "This is the primary interface to running squ wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas, json, logging, time, requests, io, pkgutil, httpx_cache\n",
    "from nbdev_squ.core import *\n",
    "from pathlib import Path\n",
    "from diskcache import memoize_stampede\n",
    "from importlib.metadata import version\n",
    "from subprocess import run, CalledProcessError\n",
    "from azure.monitor.query import LogsQueryClient, LogsBatchQuery, LogsQueryStatus\n",
    "from azure.identity import AzureCliCredential\n",
    "from benedict import benedict\n",
    "from functools import cached_property\n",
    "from atlassian import Jira\n",
    "from tenable.io import TenableIO\n",
    "from dbt.adapters.duckdb.plugins import BasePlugin, SourceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Clients\n",
    "\n",
    "Returns preconfigured clients for various services, cached for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Clients:\n",
    "    \"\"\"\n",
    "    Clients for various services, cached for performance\n",
    "    \"\"\"\n",
    "    @cached_property\n",
    "    def config(self):\n",
    "        login()\n",
    "        return cache.get(\"config\", load_config())\n",
    "\n",
    "    @cached_property\n",
    "    def runzero(self):\n",
    "        \"\"\"\n",
    "        Returns a runzero client\n",
    "        \"\"\"\n",
    "        return httpx_cache.Client(base_url=\"https://console.rumble.run/api/v1.0\", headers={\"Authorization\": f\"Bearer {self.config.runzero_apitoken}\"})\n",
    "\n",
    "    @cached_property\n",
    "    def abuseipdb(self):\n",
    "        \"\"\"\n",
    "        Returns an abuseipdb client\n",
    "        \"\"\"\n",
    "        from abuseipdb_wrapper import AbuseIPDB\n",
    "        return AbuseIPDB(api_key=self.config.abuseipdb_api_key)\n",
    "\n",
    "    @cached_property\n",
    "    def jira(self):\n",
    "        \"\"\"\n",
    "        Returns a jira client\n",
    "        \"\"\"\n",
    "        return Jira(url=self.config.jira_url, username=self.config.jira_username, password=self.config.jira_password)\n",
    "\n",
    "    @cached_property\n",
    "    def tio(self):\n",
    "        \"\"\"\n",
    "        Returns a TenableIO client\n",
    "        \"\"\"\n",
    "        return TenableIO(self.config.tenable_access_key, self.config.tenable_secret_key)\n",
    "\n",
    "\n",
    "clients = Clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunZero\n",
    "response = clients.runzero.get(\"/export/org/assets.csv\", params={\"search\": \"has_public:t AND alive:t AND (protocol:rdp OR protocol:vnc OR protocol:teamviewer OR protocol:telnet OR protocol:ftp)\"})\n",
    "pandas.read_csv(io.StringIO(response.text)).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jira\n",
    "pandas.json_normalize(clients.jira.jql(\"updated > -1d\")[\"issues\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AbuseIPDB\n",
    "clients.abuseipdb.check_ip(\"1.1.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TenableIO\n",
    "pandas.DataFrame(clients.tio.scans.list()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Workspaces\n",
    "\n",
    "The `list_workspaces` function retreives a list of workspaces from blob storage and returns it in various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_workspaces_safe(fmt: str = \"df\", # df, csv, json, list\n",
    "                    agency: str = \"ALL\"): # Agency alias or ALL\n",
    "    path = datalake_path()\n",
    "    df = pandas.read_csv((path / \"notebooks/lists/SentinelWorkspaces.csv\").open())\n",
    "    df = df.join(pandas.read_csv((path / \"notebooks/lists/SecOps Groups.csv\").open()).set_index(\"Alias\"), on=\"SecOps Group\", rsuffix=\"_secops\")\n",
    "    df = df.rename(columns={\"SecOps Group\": \"alias\", \"Domains and IPs\": \"domains\"})\n",
    "    df = df.dropna(subset=[\"customerId\"]).sort_values(by=\"alias\").convert_dtypes().reset_index()\n",
    "    persisted = f\"{dirs.user_cache_dir}/list_workspaces.parquet\"\n",
    "    df.to_parquet(persisted)\n",
    "    return persisted\n",
    "\n",
    "def list_workspaces(fmt: str = \"df\", # df, csv, json, list\n",
    "                    agency: str = \"ALL\"): # Agency alias or ALL\n",
    "    df = pandas.read_parquet(list_workspaces_safe(fmt, agency))\n",
    "    if agency != \"ALL\":\n",
    "        df = df[df[\"alias\"] == agency]\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    elif fmt == \"list\":\n",
    "        return list(df[\"customerId\"].unique())\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use it do lookup an agencies alias based on its `customerId` (also known as `TenantId`, Log Analytics `WorkspaceId`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces = list_workspaces()\n",
    "workspaces.query(f'customerId == \"{workspaces[\"customerId\"][0]}\"')[\"alias\"].str.cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analytics Query\n",
    "The below function makes it easy to query all workspaces with sentinel installed using log analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def list_subscriptions():\n",
    "    return pandas.DataFrame(azcli([\"account\", \"list\"]))[\"id\"].unique()\n",
    "\n",
    "@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\n",
    "def list_securityinsights_safe():\n",
    "    return azcli([\n",
    "        \"graph\", \"query\", \"--first\", \"1000\", \"-q\", \n",
    "        \"\"\"\n",
    "        resources\n",
    "        | where type =~ 'microsoft.operationsmanagement/solutions'\n",
    "        | where name startswith 'SecurityInsights'\n",
    "        | project wlid = tolower(tostring(properties.workspaceResourceId))\n",
    "        | join kind=leftouter (\n",
    "            resources | where type =~ 'microsoft.operationalinsights/workspaces' | extend wlid = tolower(id))\n",
    "            on wlid\n",
    "        | extend customerId = properties.customerId\n",
    "        \"\"\"\n",
    "    ])[\"data\"]\n",
    "\n",
    "def list_securityinsights():\n",
    "    return pandas.DataFrame(list_securityinsights_safe())\n",
    "\n",
    "def chunks(items, size):\n",
    "    # Yield successive `size` chunks from `items`\n",
    "    for i in range(0, len(items), size):\n",
    "        yield items[i:i + size]\n",
    "\n",
    "def loganalytics_query(queries: list[str], timespan=pandas.Timedelta(\"14d\"), batch_size=190, batch_delay=32, sentinel_workspaces = None):\n",
    "    \"\"\"\n",
    "    Run queries across all workspaces, in batches of `batch_size` with a minimum delay of `batch_delay` between batches.\n",
    "    Returns a dictionary of queries and results\n",
    "    \"\"\"\n",
    "    client = LogsQueryClient(AzureCliCredential())\n",
    "    workspaces = list_workspaces(fmt=\"df\")\n",
    "    if sentinel_workspaces is None:\n",
    "        sentinel_workspaces = list_securityinsights()\n",
    "    requests, results = [], []\n",
    "    for query in queries:\n",
    "        for workspace_id in sentinel_workspaces[\"customerId\"]:\n",
    "            requests.append(LogsBatchQuery(workspace_id=workspace_id, query=query, timespan=timespan))\n",
    "    querytime = pandas.Timestamp(\"now\")\n",
    "    logger.info(f\"Executing {len(requests)} queries at {querytime}\")\n",
    "    for request_batch in chunks(requests, batch_size):\n",
    "        while cache.get(\"loganalytics_query_running\"):\n",
    "            time.sleep(1)\n",
    "        batch_start = pandas.Timestamp(\"now\")\n",
    "        cache.set(\"loganalytics_query_running\", True, batch_delay)\n",
    "        results += client.query_batch(request_batch)\n",
    "        duration = pandas.Timestamp(\"now\") - batch_start\n",
    "        logger.info(f\"Completed {len(results)} (+{len(request_batch)}) in {duration}\")\n",
    "    dfs = {}\n",
    "    for request, result in zip(requests, results):\n",
    "        if result.status == LogsQueryStatus.PARTIAL:\n",
    "            tables = result.partial_data\n",
    "            tables = [pandas.DataFrame(table.rows, columns=table.columns) for table in tables]\n",
    "        elif result.status == LogsQueryStatus.SUCCESS:\n",
    "            tables = result.tables\n",
    "            tables = [pandas.DataFrame(table.rows, columns=table.columns) for table in tables]\n",
    "        else:\n",
    "            tables = [pandas.DataFrame([result.__dict__])]\n",
    "        df = pandas.concat(tables).dropna(axis=1, how='all') # prune empty columns\n",
    "        df[\"TenantId\"] = request.workspace\n",
    "        alias = workspaces.query(f'customerId == \"{request.workspace}\"')[\"alias\"].str.cat()\n",
    "        if alias == '':\n",
    "            alias = sentinel_workspaces.query(f'customerId == \"{request.workspace}\"')[\"name\"].str.cat()\n",
    "        df[\"_alias\"] = alias\n",
    "        query = request.body[\"query\"]\n",
    "        if query in dfs:\n",
    "            dfs[query].append(df)\n",
    "        else:\n",
    "            dfs[query] = [df]\n",
    "    return {query: pandas.concat(results, ignore_index=True).convert_dtypes() for query, results in dfs.items()}\n",
    "\n",
    "def query_all(query, fmt=\"df\", timespan=pandas.Timedelta(\"14d\")):\n",
    "    try:\n",
    "        # Check query is not a plain string and is iterable\n",
    "        assert not isinstance(query, str)\n",
    "        iter(query)\n",
    "    except (AssertionError, TypeError):\n",
    "        # if it is a plain string or it's not iterable, convert into a list of queries\n",
    "        query = [query]\n",
    "    df = pandas.concat(loganalytics_query(query, timespan).values())\n",
    "    if fmt == \"df\":\n",
    "        return df\n",
    "    elif fmt == \"csv\":\n",
    "        return df.to_csv()\n",
    "    elif fmt == \"json\":\n",
    "        return df.fillna(\"\").to_dict(\"records\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_all([\"SecurityAlert\"]).groupby(\"_alias\")[[\"Tactics\", \"TenantId\"]].nunique().sort_values(by=\"Tactics\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat hunting helper\n",
    "\n",
    "Scan across all databases and tables for columns with a given predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "columns_of_interest = benedict({\n",
    "    \"name\": ['AADEmail', 'AccountName', 'AccountUPN', 'AccountUpn', 'Account', 'CompromisedEntity', 'DestinationUserName', \n",
    "             \"Computer\", \"DisplayName\", \"EmailSenderAddress\", \"FileName\", 'FilePath', \"FolderPath\", 'FullyQualifiedSubjectUserName', 'InitiatingProcessAccountUpn',\n",
    "            'MailboxOwnerUPN', 'Name', 'NewProcessName', 'Owner', 'ParentProcessName', 'Process', 'CommandLine', 'ProcessCommandLine', 'RecipientEmailAddress',\n",
    "            'RequesterUpn', 'SenderMailFromAddress', 'SourceIdentity', 'SourceUserName', 'SubjectUserName', 'TargetUserName', 'TargetUser', 'Upn',\n",
    "            'UserName', 'userName', 'UserPrincipalName'],\n",
    "    \"guid\": ['Caller', \"DestinationUserID\", 'SourceUserID', 'UserId'],\n",
    "    \"ip\": ['CallerIpAddress', 'DestinationIP', 'DstIpAddr', 'EmailSourceIpAddress', 'IPAddresses', 'IPAddress', 'IpAddress',\n",
    "           'NetworkDestinationIP', 'NetworkIP', 'NetworkSourceIP', 'RemoteIP', 'SourceIP', 'SrcIpAddr'],\n",
    "    \"url\": [\"DomainName\", 'FileOriginUrl', 'FQDN', 'RemoteUrl', 'Url'],\n",
    "    \"hash\": [\"MD5\", \"SHA1\", \"SHA256\", 'FileHashValue', 'FileHash', 'InitiatingProcessSHA256']\n",
    "})\n",
    "\n",
    "columns = [column for area in columns_of_interest.values() for column in area]\n",
    "\n",
    "def finalise_query(query, take):\n",
    "    return f\"{query} | take {take} | extend placeholder_=dynamic({{'':null}}) | evaluate bag_unpack(column_ifexists('pack_', placeholder_))\"\n",
    "\n",
    "def hunt(indicators, expression=\"has\", columns=columns, workspaces=None, timespans=[\"1d\", \"14d\", \"90d\", \"700d\"], take=5000):\n",
    "    queries = []\n",
    "    if workspaces is None:\n",
    "        workspaces = list_securityinsights()\n",
    "    else:\n",
    "        df = list_securityinsights()\n",
    "        workspaces = df[df[\"customerId\"].isin(workspaces)]\n",
    "    querylogged = False\n",
    "    if expression in ['has_any']:\n",
    "        query = f\"let indicators = dynamic({indicators}); \"\n",
    "\n",
    "        for count, column in enumerate(columns):\n",
    "            if count == 0:\n",
    "                query += f\"find where {column} has_any (indicators)\"\n",
    "            else:\n",
    "                query += f\" or {column} has_any (indicators)\"\n",
    "        final_query = finalise_query(query, take)\n",
    "        queries.append(final_query)\n",
    "    else:\n",
    "        for indicator in indicators:\n",
    "            if expression not in ['has_all']:\n",
    "                indicator = f\"'{indicator}'\" # wrap indicator in quotes unless expecting dynamic\n",
    "            if not querylogged:\n",
    "                logger.info(f\"Test Query: find where {columns[0]} {expression} {indicator} | take {take}\")\n",
    "                querylogged = True\n",
    "            for chunk in chunks([f\"{column} {expression} {indicator}\" for column in columns], 20):\n",
    "                query = \" or \".join(chunk)\n",
    "                final_query = finalise_query(f\"find where {query}\", take)\n",
    "                queries.append(final_query)\n",
    "    for timespan in timespans:\n",
    "        results = pandas.concat(loganalytics_query(queries, pandas.Timedelta(timespan), sentinel_workspaces = workspaces).values())\n",
    "        if 'placeholder_' in results.columns:\n",
    "            results = results.drop('placeholder_', axis=1)\n",
    "        if results.empty:\n",
    "            logger.info(f\"No results in {timespan}, extending hunt\")\n",
    "            continue\n",
    "        logger.info(f\"Found {indicators} in {timespan}, returning\")\n",
    "        return results\n",
    "    else:\n",
    "        raise Exception(\"No results found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hunt(['(\"ntdsutil\", \"ifm\")'], expression=\"has_all\", columns=[\"InitiatingProcessCommandLine\", \"ProcessCommandLine\", \"CommandLine\"], timespans=[\"90d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def atlaskit_transformer(inputtext, inputfmt=\"md\", outputfmt=\"wiki\", runtime=\"node\"):\n",
    "    transformer = dirs.user_cache_path / f\"atlaskit-transformer.bundle_v{version('nbdev_squ')}.js\"\n",
    "    if not transformer.exists():\n",
    "        transformer.write_bytes(pkgutil.get_data(\"nbdev_squ\", \"atlaskit-transformer.bundle.js\"))\n",
    "    cmd = [runtime, str(transformer), inputfmt, outputfmt]\n",
    "    logger.debug(\" \".join(cmd))\n",
    "    try:\n",
    "        return run(cmd, input=inputtext, text=True, capture_output=True, check=True).stdout\n",
    "    except CalledProcessError:\n",
    "        run(cmd, input=inputtext, text=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(atlaskit_transformer(\"\"\"# Heading 1\n",
    "\n",
    "- a bullet\n",
    "[a link](https://github.com]\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel Incident sync helpers\n",
    "\n",
    "Overall process below will be to grab all the incidents from Sentinel, and save them into individual files per day based on their last updated time. A subsequent process can then load those files (e.g. ingest into ADX) and/or synchronise the updates into Jira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def security_incidents(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"1d\"), timedelta=pandas.Timedelta(\"1d\")):\n",
    "    # Queries for security incidents from `start` time for `timedelta` and returns a dataframe\n",
    "    # Sorts by TimeGenerated (TODO)\n",
    "    query = \"SecurityIncident | summarize arg_max(TimeGenerated, *) by IncidentNumber\"\n",
    "    return query_all(query, timespan=(start.to_pydatetime(), timedelta))\n",
    "\n",
    "def security_alerts(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"1d\"), timedelta=pandas.Timedelta(\"1d\")):\n",
    "    # Queries for security alerts from `start` time for `timedelta` and returns a dataframe\n",
    "    # Sorts by TimeGenerated (TODO)\n",
    "    query = \"SecurityAlert | summarize arg_max(TimeGenerated, *) by SystemAlertId\"\n",
    "    return query_all(query, timespan=(start.to_pydatetime(), timedelta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = security_incidents(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"14d\"), timedelta=pandas.Timedelta(\"14d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dbt-duckdb plugin\n",
    "\n",
    "The below squ plugin makes querying kql in duckdb projects via the [DuckDB user-defined function (UDF)](https://duckdb.org/docs/api/python/function.html) interface much easier. This could be extended to other clients pretty easily, just have to make sure data is returned as a dataframe. To use it there are a few dbt project files that need to be configured:\n",
    "\n",
    "### DBT ./profiles.yml\n",
    "\n",
    "See [DBT Connection Profiles](https://docs.getdbt.com/docs/core/connect-data-platform/connection-profiles)\n",
    "\n",
    "```yaml\n",
    "default:\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: duckdb\n",
    "      path: target/dev.duckdb\n",
    "      plugins: \n",
    "        - module: nbdev_squ.api\n",
    "          alias: squ\n",
    "\n",
    "  target: dev\n",
    "```\n",
    "\n",
    "### DBT ./models/squ/schema.yml\n",
    "\n",
    "See [DBT Add sources to your DAG](https://docs.getdbt.com/docs/build/sources) for how to add 'externally defined' sources, this is using the code below based on the [dbt-duckdb plugin](https://github.com/duckdb/dbt-duckdb?tab=readme-ov-file#writing-your-own-plugins) architecture\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: kql_source\n",
    "    config:\n",
    "      plugin: squ\n",
    "    meta:\n",
    "      kql_path: \"models/squ/{name}.kql\"\n",
    "    tables:\n",
    "    - name: T1547_001\n",
    "\n",
    "models:\n",
    "  - name: hunt\n",
    "    config:\n",
    "      materialized: table\n",
    "```\n",
    "\n",
    "Once the source is defined, dbt cli tools and other sql models can refer to it, the dbt-duckdb framework makes it available as a referencable view usable throughout the project:\n",
    "\n",
    "#### DBT ./models/squ/hunt.sql\n",
    "\n",
    "See [DBT SQL models](https://docs.getdbt.com/docs/build/sql-models) for how to write the select statement templates DBT organises into the DAG\n",
    "\n",
    "```sql\n",
    "select * from {{source('kql_source', 'T1547_001')}}\n",
    "```\n",
    "\n",
    "#### DBT cli usage\n",
    "\n",
    "See [About the dbt run command](https://docs.getdbt.com/reference/commands/run) (can use `--empty` to validate before a full run)\n",
    "\n",
    "```bash\n",
    "cd dbt_example_project\n",
    "dbt run # will build the whole dag including any materialisations like the hunt table above\n",
    "dbt show --inline \"select * from {{ source('kql_source', 'T1547_001') }}\" # will use live source\n",
    "dbt show --inline \"select * from {{ ref('hunt') }}\" # will use materialized table in db built by `dbt run`\n",
    "dbt docs generate # will build documentation for the project\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Plugin(BasePlugin):\n",
    "    def initialize(self, config):\n",
    "        login()\n",
    "\n",
    "    def configure_cursor(self, cursor):\n",
    "        pass\n",
    "\n",
    "    def load(self, source_config: SourceConfig):\n",
    "        if \"kql_path\" in source_config:\n",
    "            kql_path = source_config[\"kql_path\"]\n",
    "            kql_path = kql_path.format(**source_config.as_dict())\n",
    "            query = Path(kql_path).read_text()\n",
    "            return query_all(query, timespan=pandas.Timedelta(source_config.get(\"timespan\", \"14d\")))\n",
    "            raise Exception(\"huh\")\n",
    "        elif \"list_workspaces\" in source_config: # untested\n",
    "            return list_workspaces()\n",
    "        elif \"client_api\" in source_config: # untested\n",
    "            api_result = getattr(clients, source_config[\"client_api\"])(**json.loads(source_config.get(\"kwargs\", \"{}\")))\n",
    "            if isinstance(api_result, pandas.DataFrame):\n",
    "                return api_result\n",
    "            else:\n",
    "                return pandas.DataFrame(api_result)\n",
    "        else:\n",
    "            raise Exception(\"No valid config found for squ plugin (kql_path or api required)\")\n",
    "\n",
    "    def default_materialization(self):\n",
    "        return \"view\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
