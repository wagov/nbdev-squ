[
  {
    "objectID": "legacy.html",
    "href": "legacy.html",
    "title": "legacy",
    "section": "",
    "text": "logging.basicConfig(level=logging.INFO)\n\n\n@memoize_stampede(api.cache, expire=60 * 5) # cache for 5 mins\ndef adx_query(kql):\n    \"\"\"\n    Run a kusto query\n\n    Args:\n        kql (str or list): kusto query or list of queries\n\n    Returns:\n        json: query results\n    \"\"\"\n    if isinstance(kql, list):\n        kql = [\".execute script with (ContinueOnErrors=true) &lt;|\"] + kql\n        kql = \"\\n\".join(kql)\n    config = api.cache[\"config\"]\n    cluster, dx_db = config.azure_dataexplorer.rsplit(\"/\", 1)\n    dx_client = KustoClient(KustoConnectionStringBuilder.with_az_cli_authentication(cluster))\n    return dx_client.execute(dx_db, kql.replace(\"\\\\\", \"\\\\\\\\\")).primary_results[0]\n\ndef adxtable2df(table):\n    \"\"\"\n    Return a pandas dataframe from an adx table\n    \"\"\"\n    columns = [col.column_name for col in table.columns]\n    frame = pandas.DataFrame(table.raw_rows, columns=columns)\n    return frame\n\n\nsource\n\nadxtable2df\n\n adxtable2df (table)\n\nReturn a pandas dataframe from an adx table\n\nsource\n\n\nadx_query\n\n adx_query (kql)\n\nRun a kusto query\nArgs: kql (str or list): kusto query or list of queries\nReturns: json: query results\n\n\n\n\nDetails\n\n\n\n\nkql\ncache for 5 mins\n\n\n\n\nadxtable2df(adx_query(\"SecurityAlert | take 10\"))\n\n\ndef export_jira_issues():\n    \"\"\"\n    Exports all JIRA issues to the data lake.\n    \"\"\"\n    jira_issues_path = api.datalake_path() / \"jira_outputs\" / \"issues\"\n\n    def getissues(start_at, jql):\n        response = api.clients.jira.jql(jql, start=start_at, limit=100)\n        next_start = response[\"startAt\"] + response[\"maxResults\"]\n        total_rows = response[\"total\"]\n        if next_start &gt; total_rows:\n            next_start = total_rows\n        issues = response[\"issues\"]\n        return next_start, total_rows, issues\n\n    def save_date_issues(after_date: pandas.Timestamp, path=jira_issues_path):\n        fromdate = after_date\n        todate = after_date + pandas.to_timedelta(\"1d\")\n        jql = f\"updated &gt;= {fromdate.date().isoformat()} and updated &lt; {todate.date().isoformat()} order by key\"\n        output = path / f\"{fromdate.date().isoformat()}\" / \"issues.parquet\"\n        if output.exists() and fromdate &lt; pandas.Timestamp.now() - pandas.to_timedelta(\"1d\"):\n            # skip previously dumped days except for last day\n            return None\n        start_at, total_rows = 0, -1\n        dataframes = []\n        while start_at != total_rows:\n            start_at, total_rows, issues = getissues(start_at, jql)\n            dataframes.append(pandas.DataFrame(issues))\n            if start_at == 100:\n                logger.info(f\"{total_rows} to load\")\n        if total_rows &gt; 1:\n            df = pandas.concat(dataframes)\n            df[\"fields\"] = df[\"fields\"].apply(json.dumps)\n            logger.info(f\"saving {output}\")\n            try:\n                df.to_parquet(output.open(\"wb\"))\n            except Exception as exc:\n                print(exc)\n            return df\n        else:\n            return None\n\n    after = pandas.Timestamp.now() - pandas.to_timedelta(\"7d\")\n    until = pandas.Timestamp.now() + pandas.to_timedelta(\"1d\")\n\n    while after &lt; until:\n        save_date_issues(after)\n        after += pandas.to_timedelta(\"1d\")\n\n\nsource\n\n\nexport_jira_issues\n\n export_jira_issues ()\n\nExports all JIRA issues to the data lake.\n\nexport_jira_issues()\n\n\ndef flatten(nested_dict, parent_key='', sep='_'):\n    \"\"\"\n    Flatten a nested dictionary.\n    \n    Args:\n        nested_dict (dict): The nested dictionary to flatten.\n        parent_key (str, optional): The parent key for the current level of nesting.\n        sep (str, optional): The separator to use for flattened keys.\n    \n    Returns:\n        dict: The flattened dictionary.\n    \"\"\"\n    flat_dict = {}\n    \n    for key, value in nested_dict.items():\n        new_key = f\"{parent_key}{sep}{key}\" if parent_key else key\n        \n        if isinstance(value, dict):\n            flat_dict.update(flatten(value, new_key, sep))\n        else:\n            flat_dict[new_key] = value\n    \n    return flat_dict\n\ndef sentinel_beautify_local(\n    data: dict,\n    outputformat: str = \"jira\",\n    default_status: str = \"Onboard: MOU (T0)\",\n    default_orgid: int = 2,\n):\n    \"\"\"\n    Takes a SecurityIncident including alerts as json and returns\n    markdown, html and detailed json representation.\n    \"\"\"\n    for jsonfield in [\"Labels\", \"Owner\", \"AdditionalData\", \"Comments\"]:\n        if data.get(jsonfield):\n            data[jsonfield] = json.loads(data[jsonfield])\n    labels = [\n        f\"SIEM_Severity:{data['Severity']}\",\n        f\"SIEM_Status:{data['Status']}\",\n        f\"SIEM_Title:{data['Title']}\",\n    ]\n    labels += [l[\"labelName\"] for l in data[\"Labels\"]]  # copy over labels from incident\n    incident_details = [data[\"Description\"], \"\"]\n\n    if data.get(\"Owner\"):\n        owner = None\n        if data[\"Owner\"].get(\"email\"):\n            owner = data[\"Owner\"][\"email\"]\n        elif data[\"Owner\"].get(\"userPrincipalName\"):\n            owner = data[\"Owner\"][\"userPrincipalName\"]\n        if owner:\n            labels.append(f\"SIEM_Owner:{owner}\")\n            incident_details.append(f\"- **Sentinel Incident Owner:** {owner}\")\n\n    if data.get(\"Classification\"):\n        labels.append(f\"SIEM_Classification:{data['Classification']}\")\n        incident_details.append(f\"- **Alert Classification:** {data['Classification']}\")\n\n    if data.get(\"ClassificationReason\"):\n        labels.append(f\"SIEM_ClassificationReason:{data['ClassificationReason']}\")\n        incident_details.append(\n            f\"- **Alert Classification Reason:** {data['ClassificationReason']}\"\n        )\n\n    if data.get(\"ProviderName\"):\n        labels.append(f\"SIEM_ProviderName:{data['ProviderName']}\")\n        incident_details.append(f\"- **Provider Name:** {data['ProviderName']}\")\n\n    if data.get(\"AdditionalData\"):\n        if data[\"AdditionalData\"].get(\"alertProductNames\"):\n            product_names = \",\".join(data[\"AdditionalData\"][\"alertProductNames\"])\n            labels.append(f\"SIEM_alertProductNames:{product_names}\")\n            incident_details.append(f\"- **Product Names:** {product_names}\")\n        if data[\"AdditionalData\"].get(\"tactics\"):\n            tactics = \",\".join(data[\"AdditionalData\"][\"tactics\"])\n            labels.append(f\"SIEM_tactics:{tactics}\")\n            incident_details.append(\n                f\"- **[MITRE ATT&CK Tactics](https://attack.mitre.org/tactics/):** {tactics}\"\n            )\n        if data[\"AdditionalData\"].get(\"techniques\"):\n            techniques = \",\".join(data[\"AdditionalData\"][\"techniques\"])\n            labels.append(f\"SIEM_techniques:{techniques}\")\n            incident_details.append(\n                \"- **[MITRE ATT&CK Techniques](https://attack.mitre.org/techniques/):**\"\n                f\" {techniques}\"\n            )\n\n    comments = []\n    if data.get(\"Comments\"):\n        if len(data[\"Comments\"]) &gt; 0:\n            comments += [\"\", \"## Comments\"]\n            for comment in data[\"Comments\"]:\n                comments += comment[\"message\"].split(\"\\n\")\n            comments += [\"\"]\n\n    alert_details = []\n    observables = []\n    entity_type_value_mappings = {\n        \"host\": \"{HostName}\",\n        \"account\": \"{Name}\",\n        \"process\": \"{CommandLine}\",\n        \"file\": \"{Name}\",\n        \"ip\": \"{Address}\",\n        \"url\": \"{Url}\",\n        \"dns\": \"{DomainName}\",\n        \"registry-key\": \"{Hive}{Key}\",\n        \"filehash\": \"{Algorithm}{Value}\",\n    }\n\n    class Default(dict):\n        \"\"\"\n        Default dict that returns the key if the key is not found\n        Args:\n            dict\n        \"\"\"\n\n        def __missing__(self, key):\n            return key\n\n    for alert in data[\"AlertData\"][:10]:  # Assumes alertdata is newest to oldest\n        if not alert_details:\n            alert_details += [\n                \"\",\n                \"## Alert Details\",\n                (\n                    \"The last day of activity (up to 10 alerts) is summarised below from\"\n                    \" newest to oldest.\"\n                ),\n            ]\n        alert_details.append(\n            f\"### [{alert['AlertName']} (Severity:{alert['AlertSeverity']}) - \"\n            + f\"TimeGenerated {alert['TimeGenerated']}]({alert['AlertLink']})\"\n        )\n        alert_details.append(alert[\"Description\"])\n        for key in [\n            \"RemediationSteps\",\n            \"ExtendedProperties\",\n            \"Entities\",\n        ]:  # entities last as may get truncated\n            if alert.get(key):\n                if isinstance(alert[key], str) and alert[key][0] in [\"{\", \"[\"]:\n                    alert[key] = json.loads(alert[key])\n                if key == \"Entities\":  # add the entity to our list of observables\n                    for entity in alert[key]:\n                        observable = {\"value\": None}\n                        if \"Type\" in entity:\n                            observable = {\n                                \"type\": entity[\"Type\"],\n                                \"value\": entity_type_value_mappings.get(\n                                    entity[\"Type\"], \"\"\n                                ).format_map(Default(entity)),\n                            }\n                        if not observable[\"value\"]:  # dump whole dict as string if no mapping found\n                            observable[\"value\"] = repr(entity)\n                        observables.append(observable)\n                if alert[key] and isinstance(alert[key], list) and isinstance(alert[key][0], dict):\n                    # if list of dicts, make a table\n                    for index, entry in enumerate(\n                        [flatten(item) for item in alert[key] if len(item.keys()) &gt; 1]\n                    ):\n                        alert_details += [\"\", f\"#### {key}.{index}\"]\n                        for entrykey, value in entry.items():\n                            if value:\n                                alert_details.append(f\"- **{entrykey}:** {value}\")\n                elif isinstance(alert[key], dict):  # if dict display as list\n                    alert_details += [\"\", f\"#### {key}\"]\n                    for entrykey, value in alert[key].items():\n                        if value and len(value) &lt; 200:\n                            alert_details.append(f\"- **{entrykey}:** {value}\")\n                        elif value:  # break out long blocks\n                            alert_details += [f\"- **{entrykey}:**\", \"\", \"```\", value, \"```\", \"\"]\n                else:  # otherwise just add as separate lines\n                    alert_details += [\"\", f\"#### {key}\"] + [item for item in alert[key]]\n\n    title = (\n        f\"SIEM Detection #{data['IncidentNumber']} Sev:{data['Severity']} -\"\n        f\" {data['Title']} (Status:{data['Status']})\"\n    )\n    mdtext = (\n        [\n            f\"# {title}\",\n            \"\",\n            f\"## [SecurityIncident #{data['IncidentNumber']} Details]({data['IncidentUrl']})\",\n            \"\",\n        ]\n        + incident_details\n        + comments\n        + alert_details\n    )\n    mdtext = \"\\n\".join([str(line) for line in mdtext])\n    content = markdown(mdtext, extensions=[\"tables\"])\n    # remove special chars and deduplicate labels\n    labels = set(\"\".join(c for c in label if c.isalnum() or c in \".:_\") for label in labels)\n\n    response = {\n        \"subject\": title,\n        \"labels\": list(labels),\n        \"observables\": [dict(ts) for ts in set(tuple(i.items()) for i in observables)],\n        \"sentinel_data\": data,\n    }\n    workspaces_df = api.list_workspaces()\n    customer = (\n        workspaces_df[workspaces_df[\"customerId\"] == data[\"TenantId\"]].to_dict(\"records\")\n    )\n    if len(customer) &gt; 0:\n        customer = customer[0]\n    else:\n        customer = {}\n    # Grab wiki format for jira and truncate to 32767 chars\n    response.update(\n        {\n            \"secops_status\": customer.get(\"SecOps Status\") or default_status,\n            \"jira_orgid\": customer.get(\"JiraOrgId\") or default_orgid,\n            \"customer\": customer,\n            \"wikimarkup\": (\n                api.atlaskit_transformer(mdtext)[:32760]\n            ),\n        }\n    )\n    return response\n\n\nsource\n\n\nsentinel_beautify_local\n\n sentinel_beautify_local (data:dict, outputformat:str='jira',\n                          default_status:str='Onboard: MOU (T0)',\n                          default_orgid:int=2)\n\nTakes a SecurityIncident including alerts as json and returns markdown, html and detailed json representation.\n\nsource\n\n\nflatten\n\n flatten (nested_dict, parent_key='', sep='_')\n\nFlatten a nested dictionary.\nArgs: nested_dict (dict): The nested dictionary to flatten. parent_key (str, optional): The parent key for the current level of nesting. sep (str, optional): The separator to use for flattened keys.\nReturns: dict: The flattened dictionary.\n\n# grab latest incident with alerts\nincident = api.security_incidents().dropna(subset=[\"AlertIds\"]).iloc[0]\ndf = api.security_alerts()\ndf = df[df[\"TenantId\"] == incident[\"TenantId\"]]\nalertids = json.loads(incident[\"AlertIds\"])\n# extend incident with alert info\nincident[\"AlertData\"] = df[df[\"SystemAlertId\"].isin(alertids)].copy(deep=True).to_dict(orient=\"records\")\n\n\n# convert to a jira friendly format\nsentinel_beautify_local(incident.to_dict())",
    "crumbs": [
      "legacy"
    ]
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "api",
    "section": "",
    "text": "logging.basicConfig(level=logging.INFO)",
    "crumbs": [
      "api"
    ]
  },
  {
    "objectID": "api.html#list-workspaces",
    "href": "api.html#list-workspaces",
    "title": "api",
    "section": "List Workspaces",
    "text": "List Workspaces\nThe list_workspaces function retreives a list of workspaces from blob storage and returns it in various formats\n\n@memoize_stampede(cache, expire=60 * 60 * 3) # cache for 3 hours\ndef list_workspaces(fmt: str = \"df\", # df, csv, json, list\n                    agency: str = \"ALL\"): # Agency alias or ALL\n    path = datalake_path()\n    df = pandas.read_csv((path / \"notebooks/lists/SentinelWorkspaces.csv\").open())\n    df = df.join(pandas.read_csv((path / \"notebooks/lists/SecOps Groups.csv\").open()).set_index(\"Alias\"), on=\"SecOps Group\", rsuffix=\"_secops\")\n    df = df.rename(columns={\"SecOps Group\": \"alias\", \"Domains and IPs\": \"domains\"})\n    df = df.dropna(subset=[\"customerId\"]).sort_values(by=\"alias\").convert_dtypes().reset_index()\n    if agency != \"ALL\":\n        df = df[df[\"alias\"] == agency]\n    if fmt == \"df\":\n        return df\n    elif fmt == \"csv\":\n        return df.to_csv()\n    elif fmt == \"json\":\n        return df.fillna(\"\").to_dict(\"records\")\n    elif fmt == \"list\":\n        return list(df[\"customerId\"].unique())\n    else:\n        raise ValueError(\"Invalid format\")\n\n\nsource\n\nlist_workspaces\n\n list_workspaces (fmt:str='df', agency:str='ALL')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfmt\nstr\ndf\ndf, csv, json, list\n\n\nagency\nstr\nALL\nAgency alias or ALL\n\n\n\nYou can use it do lookup an agencies alias based on its customerId (also known as TenantId, Log Analytics WorkspaceId)\n\nworkspaces = list_workspaces()\nworkspaces.query(f'customerId == \"{workspaces[\"customerId\"][0]}\"')[\"alias\"].str.cat()",
    "crumbs": [
      "api"
    ]
  },
  {
    "objectID": "api.html#threat-hunting-helper",
    "href": "api.html#threat-hunting-helper",
    "title": "api",
    "section": "Threat hunting helper",
    "text": "Threat hunting helper\nScan across all databases and tables for columns with a given predicate\n\ncolumns_of_interest = benedict({\n    \"name\": ['AADEmail', 'AccountName', 'AccountUPN', 'AccountUpn', 'Account', 'CompromisedEntity', 'DestinationUserName', \n             \"Computer\", \"DisplayName\", \"EmailSenderAddress\", \"FileName\", 'FilePath', \"FolderPath\", 'FullyQualifiedSubjectUserName', 'InitiatingProcessAccountUpn',\n            'MailboxOwnerUPN', 'Name', 'NewProcessName', 'Owner', 'ParentProcessName', 'Process', 'CommandLine', 'ProcessCommandLine', 'RecipientEmailAddress',\n            'RequesterUpn', 'SenderMailFromAddress', 'SourceIdentity', 'SourceUserName', 'SubjectUserName', 'TargetUserName', 'TargetUser', 'Upn',\n            'UserName', 'userName', 'UserPrincipalName'],\n    \"guid\": ['Caller', \"DestinationUserID\", 'SourceUserID', 'UserId'],\n    \"ip\": ['CallerIpAddress', 'DestinationIP', 'DstIpAddr', 'EmailSourceIpAddress', 'IPAddresses', 'IPAddress', 'IpAddress',\n           'NetworkDestinationIP', 'NetworkIP', 'NetworkSourceIP', 'RemoteIP', 'SourceIP', 'SrcIpAddr'],\n    \"url\": [\"DomainName\", 'FileOriginUrl', 'FQDN', 'RemoteUrl', 'Url'],\n    \"hash\": [\"MD5\", \"SHA1\", \"SHA256\", 'FileHashValue', 'FileHash', 'InitiatingProcessSHA256']\n})\n\ncolumns = [column for area in columns_of_interest.values() for column in area]\n\ndef finalise_query(query, take):\n    return f\"{query} | take {take} | extend placeholder_=dynamic({{'':null}}) | evaluate bag_unpack(column_ifexists('pack_', placeholder_))\"\n\ndef hunt(indicators, expression=\"has\", columns=columns, workspaces=None, timespans=[\"1d\", \"14d\", \"90d\", \"700d\"], take=5000):\n    queries = []\n    if workspaces is None:\n        workspaces = list_securityinsights()\n    else:\n        df = list_securityinsights()\n        workspaces = df[df[\"customerId\"].isin(workspaces)]\n    querylogged = False\n    if expression in ['has_any']:\n        query = f\"let indicators = dynamic({indicators}); \"\n\n        for count, column in enumerate(columns):\n            if count == 0:\n                query += f\"find where {column} has_any (indicators)\"\n            else:\n                query += f\" or {column} has_any (indicators)\"\n        final_query = finalise_query(query, take)\n        queries.append(final_query)\n    else:\n        for indicator in indicators:\n            if expression not in ['has_all']:\n                indicator = f\"'{indicator}'\" # wrap indicator in quotes unless expecting dynamic\n            if not querylogged:\n                logger.info(f\"Test Query: find where {columns[0]} {expression} {indicator} | take {take}\")\n                querylogged = True\n            for chunk in chunks([f\"{column} {expression} {indicator}\" for column in columns], 20):\n                query = \" or \".join(chunk)\n                final_query = finalise_query(f\"find where {query}\", take)\n                queries.append(final_query)\n    for timespan in timespans:\n        results = pandas.concat(loganalytics_query(queries, pandas.Timedelta(timespan), sentinel_workspaces = workspaces).values())\n        if 'placeholder_' in results.columns:\n            results = results.drop('placeholder_', axis=1)\n        if results.empty:\n            logger.info(f\"No results in {timespan}, extending hunt\")\n            continue\n        logger.info(f\"Found {indicators} in {timespan}, returning\")\n        return results\n    else:\n        raise Exception(\"No results found!\")\n\n\nsource\n\nhunt\n\n hunt (indicators, expression='has', columns=['AADEmail', 'AccountName',\n       'AccountUPN', 'AccountUpn', 'Account', 'CompromisedEntity',\n       'DestinationUserName', 'Computer', 'DisplayName',\n       'EmailSenderAddress', 'FileName', 'FilePath', 'FolderPath',\n       'FullyQualifiedSubjectUserName', 'InitiatingProcessAccountUpn',\n       'MailboxOwnerUPN', 'Name', 'NewProcessName', 'Owner',\n       'ParentProcessName', 'Process', 'CommandLine',\n       'ProcessCommandLine', 'RecipientEmailAddress', 'RequesterUpn',\n       'SenderMailFromAddress', 'SourceIdentity', 'SourceUserName',\n       'SubjectUserName', 'TargetUserName', 'TargetUser', 'Upn',\n       'UserName', 'userName', 'UserPrincipalName', 'Caller',\n       'DestinationUserID', 'SourceUserID', 'UserId', 'CallerIpAddress',\n       'DestinationIP', 'DstIpAddr', 'EmailSourceIpAddress',\n       'IPAddresses', 'IPAddress', 'IpAddress', 'NetworkDestinationIP',\n       'NetworkIP', 'NetworkSourceIP', 'RemoteIP', 'SourceIP',\n       'SrcIpAddr', 'DomainName', 'FileOriginUrl', 'FQDN', 'RemoteUrl',\n       'Url', 'MD5', 'SHA1', 'SHA256', 'FileHashValue', 'FileHash',\n       'InitiatingProcessSHA256'], workspaces=None, timespans=['1d',\n       '14d', '90d', '700d'], take=5000)\n\n\nsource\n\n\nfinalise_query\n\n finalise_query (query, take)\n\n\nhunt(['(\"ntdsutil\", \"ifm\")'], expression=\"has_all\", columns=[\"InitiatingProcessCommandLine\", \"ProcessCommandLine\", \"CommandLine\"], timespans=[\"90d\"])\n\n\ndef atlaskit_transformer(inputtext, inputfmt=\"md\", outputfmt=\"wiki\", runtime=\"node\"):\n    transformer = dirs.user_cache_path / f\"atlaskit-transformer.bundle_v{version('nbdev_squ')}.js\"\n    if not transformer.exists():\n        transformer.write_bytes(pkgutil.get_data(\"nbdev_squ\", \"atlaskit-transformer.bundle.js\"))\n    cmd = [runtime, str(transformer), inputfmt, outputfmt]\n    logger.debug(\" \".join(cmd))\n    try:\n        return run(cmd, input=inputtext, text=True, capture_output=True, check=True).stdout\n    except CalledProcessError:\n        run(cmd, input=inputtext, text=True, check=True)\n\n\nsource\n\n\natlaskit_transformer\n\n atlaskit_transformer (inputtext, inputfmt='md', outputfmt='wiki',\n                       runtime='node')\n\n\nprint(atlaskit_transformer(\"\"\"# Heading 1\n\n- a bullet\n[a link](https://github.com]\n\"\"\"))",
    "crumbs": [
      "api"
    ]
  },
  {
    "objectID": "api.html#sentinel-incident-sync-helpers",
    "href": "api.html#sentinel-incident-sync-helpers",
    "title": "api",
    "section": "Sentinel Incident sync helpers",
    "text": "Sentinel Incident sync helpers\nOverall process below will be to grab all the incidents from Sentinel, and save them into individual files per day based on their last updated time. A subsequent process can then load those files (e.g.Â ingest into ADX) and/or synchronise the updates into Jira.\n\ndef security_incidents(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"1d\"), timedelta=pandas.Timedelta(\"1d\")):\n    # Queries for security incidents from `start` time for `timedelta` and returns a dataframe\n    # Sorts by TimeGenerated (TODO)\n    query = \"SecurityIncident | summarize arg_max(TimeGenerated, *) by IncidentNumber\"\n    return query_all(query, timespan=(start.to_pydatetime(), timedelta))\n\ndef security_alerts(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"1d\"), timedelta=pandas.Timedelta(\"1d\")):\n    # Queries for security alerts from `start` time for `timedelta` and returns a dataframe\n    # Sorts by TimeGenerated (TODO)\n    query = \"SecurityAlert | summarize arg_max(TimeGenerated, *) by SystemAlertId\"\n    return query_all(query, timespan=(start.to_pydatetime(), timedelta))\n\n\nsource\n\nsecurity_alerts\n\n security_alerts (start=Timestamp('2024-03-30 14:19:54.177709+0000',\n                  tz='UTC'), timedelta=Timedelta('1 days 00:00:00'))\n\n\nsource\n\n\nsecurity_incidents\n\n security_incidents (start=Timestamp('2024-03-30 14:19:54.177552+0000',\n                     tz='UTC'), timedelta=Timedelta('1 days 00:00:00'))\n\n\ndf = security_incidents(start=pandas.Timestamp(\"now\", tz=\"UTC\") - pandas.Timedelta(\"14d\"), timedelta=pandas.Timedelta(\"14d\"))",
    "crumbs": [
      "api"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "logging.basicConfig(level=logging.INFO)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#caching-and-retry-helpers",
    "href": "core.html#caching-and-retry-helpers",
    "title": "core",
    "section": "Caching and retry helpers",
    "text": "Caching and retry helpers\nThe below cache sets up a persistent per user disk cache (to ensure security) that can be used throughout api setup and configuration. retryer will try to run a function again up to 3 times with a random exponential backoff to handle upstream api exceptions.\n\ndirs = PlatformDirs(\"nbdev-squ\")\ncache = Cache(dirs.user_cache_dir)\nretryer = Retrying(wait=wait_random_exponential(), stop=stop_after_attempt(3), reraise=True)",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#login-and-secrets-management",
    "href": "core.html#login-and-secrets-management",
    "title": "core",
    "section": "Login and secrets management",
    "text": "Login and secrets management\nThe squ library depends on authentication configured and ready to go. There are 2 paths to login used based on environment variables available. Once logged in it will attempt to populate cache[\"config\"] with secrets from a configuration keyvault.\n\ndef load_config(path = None # Path to read json config into cache from\n               ):\n    config = benedict()\n    if path:\n        config = benedict(path.read_text(), format=\"json\")\n    try:\n        _cli([\"config\", \"set\", \"extension.use_dynamic_install=yes_without_prompt\"])\n        config = benedict(_cli([\"keyvault\", \"secret\", \"show\", \n                                \"--vault-name\", cache[\"vault_name\"], \n                                \"--name\", f\"squconfig-{cache['tenant_id']}\"]).value, format=\"json\")\n    except subprocess.CalledProcessError:\n        cache.delete(\"logged_in\") # clear the logged in state\n    config.standardize()\n    return config\n\ndef login(refresh: bool=False # Force relogin\n         ):\n    if \"/\" in os.environ.get(\"SQU_CONFIG\", \"\"):\n        cache[\"vault_name\"], cache[\"tenant_id\"] = os.environ[\"SQU_CONFIG\"].split(\"/\")\n    tenant = cache.get(\"tenant_id\")\n    try:\n        _cli([\"account\", \"show\"])\n        if tenant:\n            tenant_visible = len(_cli([\"account\", \"list\"]).search(tenant)) &gt; 0\n            assert tenant_visible &gt; 0\n        cache.set(\"logged_in\", True, 60 * 60 * 3) # cache login state for 3 hrs\n    except:\n        cache.delete(\"logged_in\")\n    while not cache.get(\"logged_in\"):\n        logger.info(\"Cache doesn't look logged in, attempting login\")\n        try:\n            # See if we can login with a managed identity in under 5 secs and see the configured tenant\n            subprocess.run([\"timeout\", \"5\", sys.executable, \"-m\", \"azure.cli\", \"login\", \"--identity\", \"-o\", \"none\", \"--allow-no-subscriptions\"], check=True)\n            if tenant:\n                tenant_visible = len(_cli([\"account\", \"list\"]).search(tenant)) &gt; 0\n                assert tenant_visible &gt; 0\n        except:\n            # If managed identity unavailable, fall back on a manual login\n            if tenant:\n                tenant_scope = [\"--tenant\", tenant]\n            else:\n                tenant_scope = []\n            _cli([\"login\", *tenant_scope, \"--use-device-code\", \"--allow-no-subscriptions\", \"-o\", \"none\"], capture_output=False)\n        # Finally, validate the login once more, and set the login state\n        try:\n            _cli([\"account\", \"show\"])\n            cache.set(\"logged_in\", True, 60 * 60 * 3) # cache login state for 3 hrs\n        except subprocess.CalledProcessError:\n            cache.delete(\"logged_in\")\n    logger.info(\"Cache state is logged in\")\n    if cache.get(\"vault_name\"): # Always reload config on any login call\n        logger.info(\"Loading config from keyvault\")\n        cache[\"config\"] = load_config() # Config lasts forever, don't expire\n\n\nsource\n\nlogin\n\n login (refresh:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrefresh\nbool\nFalse\nForce relogin\n\n\n\n\nsource\n\n\nload_config\n\n load_config (path=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nNoneType\nNone\nPath to read json config into cache from\n\n\n\n\n\nHow to login\nThe login function will be called automatically if the azcli function defined below is used and the cache has no login timestamp, otherwise it can be called manually as well to refresh the keyvault config items with load_config (this directly loads a keyvault secret into the cache based on the SQU_CONFIG environment variable).\n\nlogin()\ncache[\"config\"].keys()\n\n\ndef azcli(basecmd: list[str]):\n    if not cache.get(\"logged_in\"):\n        login()\n    return _cli(basecmd)\n\n\nsource\n\n\nazcli\n\n azcli (basecmd:list[str])",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#datalake-path",
    "href": "core.html#datalake-path",
    "title": "core",
    "section": "Datalake Path",
    "text": "Datalake Path\nThe datalake_path function below, returns a UPath pathlib style object pointing to a configured datalake location in the cache.\n\ncache['config']['datalake_container']\n\n\n@memoize_stampede(cache, expire=60 * 60 * 24)\ndef datalake_path(expiry_days: int=3, # Number of days until the SAS token expires\n                  permissions: str=\"racwdlt\" # Permissions to grant on the SAS token\n                    ):\n    if not cache.get(\"logged_in\"): # Have to login to grab keyvault config\n        login()\n    expiry = pandas.Timestamp(\"now\") + pandas.Timedelta(days=expiry_days)\n    account = cache[\"config\"][\"datalake_account\"].split(\".\")[0] # Grab the account name, not the full FQDN\n    container = cache['config']['datalake_container']\n    sas = azcli([\"storage\", \"container\", \"generate-sas\", \"--auth-mode\", \"login\", \"--as-user\",\n                 \"--account-name\", account, \"--name\", container, \"--permissions\", permissions, \"--expiry\", str(expiry.date())])\n    return UPath(f\"az://{container}\", account_name=account, sas_token=sas)\n\n\nsource\n\ndatalake_path\n\n datalake_path (expiry_days:int=3, permissions:str='racwdlt')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nexpiry_days\nint\n3\nNumber of days until the SAS token expires\n\n\npermissions\nstr\nracwdlt\nPermissions to grant on the SAS token\n\n\n\n\npath = datalake_path()\nprint(\"\\n\".join([str(p) for p in path.ls()]))",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SIEM Query Utils",
    "section": "",
    "text": "Below is how to install in a plain python 3.11+ environment\npip install nbdev-squ\nThe installation can also be run in a notebook (we tend to use JupyterLab Desktop for local dev). The SQU_CONFIG env var indicates to nbdev_squ it should load the json secret squconfig-my_keyvault_tenantid from the my_kevault_name keyvault.\n%pip install nbdev-squ\nimport os; os.environ[\"SQU_CONFIG\"] = \"{{ my_keyvault_name }}/{{ my_keyvault_tenantid }}\" \n\nfrom nbdev_squ import api\n# do cool notebook stuff with api\n\n\nThe contents of the keyvault secret are loaded into memory and cached in the user_cache_dir which should be a temporary secure directory restricted to the single user. Please ensure that the system this library is used on disallows access and/or logging of the user cache directory to external locations, and is on an encrypted disk (a common approach is to use isolated VMs and workstations for sensitive activities).",
    "crumbs": [
      "SIEM Query Utils"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "SIEM Query Utils",
    "section": "",
    "text": "Below is how to install in a plain python 3.11+ environment\npip install nbdev-squ\nThe installation can also be run in a notebook (we tend to use JupyterLab Desktop for local dev). The SQU_CONFIG env var indicates to nbdev_squ it should load the json secret squconfig-my_keyvault_tenantid from the my_kevault_name keyvault.\n%pip install nbdev-squ\nimport os; os.environ[\"SQU_CONFIG\"] = \"{{ my_keyvault_name }}/{{ my_keyvault_tenantid }}\" \n\nfrom nbdev_squ import api\n# do cool notebook stuff with api\n\n\nThe contents of the keyvault secret are loaded into memory and cached in the user_cache_dir which should be a temporary secure directory restricted to the single user. Please ensure that the system this library is used on disallows access and/or logging of the user cache directory to external locations, and is on an encrypted disk (a common approach is to use isolated VMs and workstations for sensitive activities).",
    "crumbs": [
      "SIEM Query Utils"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "SIEM Query Utils",
    "section": "How to use",
    "text": "How to use\nNote: If you create/use a Github Codespace on any of the wagov repos, SQU_CONFIG should be configured automatically.\nBefore using, config needs to be loaded into squ.core.cache, which can be done automatically from json in a keyvault by setting the env var SQU_CONFIG to \"keyvault/tenantid\".\nexport SQU_CONFIG=\"{{ keyvault }}/{{ tenantid }}\"\nCan be done in python before import from nbdev_squ as well:\nimport os; os.environ[\"SQU_CONFIG\"] = \"{{ keyvault }}/{{ tenantid }}\"\n\nfrom nbdev_squ import api\nimport io, pandas\n\n# Load workspace info from datalake blob storage\ndf = api.list_workspaces(fmt=\"df\"); print(df.shape)\n\n# Load workspace info from introspection of azure graph\ndf = api.list_securityinsights(); print(df.shape)\n\n# Kusto query to Sentinel workspaces via Azure Lighthouse\ndf = api.query_all(\"SecurityIncident | take 20\", fmt=\"df\"); print(df.shape)\n\n# Kusto queries to Sentinel workspaces via Azure Lighthouse (batches up to 100 queries at a time)\ndf = api.query_all([\"SecurityAlert | take 20\" for a in range(10)]); print(df.shape)\n\n# Kusto query to ADX\n#df = api.adxtable2df(api.adx_query(\"kusto query | take 20\"))\n\n# General azure cli cmd\napi.azcli([\"config\", \"set\", \"extension.use_dynamic_install=yes_without_prompt\"])\nprint(len(api.azcli([\"account\", \"list\"])))\n\n# Various pre-configured api clients\n\n# RunZero\nresponse = api.clients.runzero.get(\"/export/org/assets.csv\", params={\"search\": \"has_public:t AND alive:t AND (protocol:rdp OR protocol:vnc OR protocol:teamviewer OR protocol:telnet OR protocol:ftp)\"})\npandas.read_csv(io.StringIO(response.text)).head(10)\n\n# Jira\npandas.json_normalize(api.clients.jira.jql(\"updated &gt; -1d\")[\"issues\"]).head(10)\n\n# AbuseIPDB\napi.clients.abuseipdb.check_ip(\"1.1.1.1\")\n\n# TenableIO\npandas.DataFrame(api.clients.tio.scans.list()).head(10)\n\n\nbadips_df = api.query_all(\"\"\"\nSecurityIncident\n| where Classification == \"TruePositive\"\n| mv-expand AlertIds\n| project tostring(AlertIds)\n| join SecurityAlert on $left.AlertIds == $right.SystemAlertId\n| mv-expand todynamic(Entities)\n| project Entities.Address\n| where isnotempty(Entities_Address)\n| distinct tostring(Entities_Address)\n\"\"\", timespan=pandas.Timedelta(\"45d\"))\n\n\ndf = api.query_all(\"find where ClientIP startswith '172.16.' | evaluate bag_unpack(pack_) | take 40000\")\n\n\ndf = api.query_all(\"\"\"union withsource=\"_table\" *\n| extend _ingestion_time_bin = bin(ingestion_time(), 1h)\n| summarize take_any(*) by _table, _ingestion_time_bin\n| project pack=pack_all(true)\"\"\")\n\n\nimport json\npandas.DataFrame(list(df[\"pack\"].apply(json.loads)))",
    "crumbs": [
      "SIEM Query Utils"
    ]
  },
  {
    "objectID": "index.html#secrets-template",
    "href": "index.html#secrets-template",
    "title": "SIEM Query Utils",
    "section": "Secrets template",
    "text": "Secrets template\nThe below json can be used as a template for saving your own json into my_keyvault_name/squconfig-my_keyvault_tenantid to use with this library:\n{\n  \"config_version\": \"20240101 - added ??? access details\",\n  \"datalake_blob_prefix\": \"https://???/???\",\n  \"datalake_subscription\": \"???\",\n  \"datalake_account\": \"???.blob.core.windows.net\",\n  \"datalake_container\": \"???\",\n  \"kql_baseurl\": \"https://raw.githubusercontent.com/???\",\n  \"azure_dataexplorer\": \"https://???.???.kusto.windows.net/???\",\n  \"tenant_id\": \"???\",\n  \"jira_url\": \"https://???.atlassian.net\",\n  \"jira_username\": \"???@???\",\n  \"jira_password\": \"???\",\n  \"runzero_apitoken\": \"???\",\n  \"abuseipdb_api_key\": \"???\",\n  \"tenable_access_key\": \"???\",\n  \"tenable_secret_key\": \"???\",\n}",
    "crumbs": [
      "SIEM Query Utils"
    ]
  }
]